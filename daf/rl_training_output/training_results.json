{
  "experiment_name": "metta_rl_training_demo",
  "total_scenarios": 4,
  "episodes_per_scenario": 50,
  "scenario_results": [
    {
      "scenario_name": "single_agent_simple",
      "description": "Single agent in simple environment",
      "episodes": 50,
      "avg_reward": 2.2118,
      "std_reward": 1.5456722679792112,
      "avg_length": 49.02,
      "success_rate": 0.02,
      "best_reward": 10.09,
      "worst_reward": -0.5000000000000002,
      "reward_improvement": true,
      "training_efficiency": 0.044236000000000004,
      "episode_rewards": [
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        4.499999999999997,
        4.499999999999997,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        -0.5000000000000002,
        -0.5000000000000002,
        10.09,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        -0.5000000000000002,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        4.499999999999997,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        -0.5000000000000002,
        4.499999999999997,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        4.499999999999997,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001
      ],
      "episode_lengths": [
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        1,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50
      ]
    },
    {
      "scenario_name": "single_agent_complex",
      "description": "Single agent in larger environment",
      "episodes": 50,
      "avg_reward": 1.7600000000000002,
      "std_reward": 2.499279896290129,
      "avg_length": 80.0,
      "success_rate": 0.0,
      "best_reward": 7.199999999999993,
      "worst_reward": -0.8000000000000005,
      "reward_improvement": true,
      "training_efficiency": 0.0,
      "episode_rewards": [
        7.199999999999993,
        3.200000000000002,
        -0.8000000000000005,
        -0.8000000000000005,
        7.199999999999993,
        -0.8000000000000005,
        -0.8000000000000005,
        -0.8000000000000005,
        -0.8000000000000005,
        -0.8000000000000005,
        -0.8000000000000005,
        -0.8000000000000005,
        3.200000000000002,
        -0.8000000000000005,
        3.200000000000002,
        3.200000000000002,
        -0.8000000000000005,
        3.200000000000002,
        7.199999999999993,
        -0.8000000000000005,
        3.200000000000002,
        -0.8000000000000005,
        3.200000000000002,
        3.200000000000002,
        3.200000000000002,
        -0.8000000000000005,
        3.200000000000002,
        -0.8000000000000005,
        3.200000000000002,
        3.200000000000002,
        3.200000000000002,
        3.200000000000002,
        3.200000000000002,
        3.200000000000002,
        3.200000000000002,
        3.200000000000002,
        -0.8000000000000005,
        3.200000000000002,
        7.199999999999993,
        3.200000000000002,
        3.200000000000002,
        -0.8000000000000005,
        -0.8000000000000005,
        3.200000000000002,
        -0.8000000000000005,
        3.200000000000002,
        -0.8000000000000005,
        -0.8000000000000005,
        3.200000000000002,
        -0.8000000000000005
      ],
      "episode_lengths": [
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80
      ]
    },
    {
      "scenario_name": "multi_agent_simple",
      "description": "Multiple agents in simple environment",
      "episodes": 50,
      "avg_reward": 5.1218,
      "std_reward": 2.3416504350564322,
      "avg_length": 98.02,
      "success_rate": 0.02,
      "best_reward": 10.09,
      "worst_reward": -1.0000000000000007,
      "reward_improvement": true,
      "training_efficiency": 0.10243600000000001,
      "episode_rewards": [
        4.000000000000003,
        4.000000000000003,
        8.999999999999991,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        8.999999999999991,
        4.000000000000003,
        4.000000000000003,
        8.999999999999991,
        8.999999999999991,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        8.999999999999991,
        4.000000000000003,
        10.09,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        -1.0000000000000007,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        8.999999999999991,
        8.999999999999991,
        8.999999999999991,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        8.999999999999991,
        4.000000000000003,
        4.000000000000003,
        8.999999999999991,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        8.999999999999991,
        4.000000000000003
      ],
      "episode_lengths": [
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        1,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100
      ]
    },
    {
      "scenario_name": "multi_agent_complex",
      "description": "Multiple agents in complex environment",
      "episodes": 50,
      "avg_reward": 6.177199999999999,
      "std_reward": 4.762546394524672,
      "avg_length": 138.08,
      "success_rate": 0.08,
      "best_reward": 13.499999999999984,
      "worst_reward": -1.500000000000001,
      "reward_improvement": true,
      "training_efficiency": 0.49417599999999995,
      "episode_rewards": [
        6.000000000000004,
        6.000000000000004,
        6.000000000000004,
        -1.500000000000001,
        6.000000000000004,
        6.000000000000004,
        13.499999999999984,
        -1.500000000000001,
        6.000000000000004,
        10.09,
        6.000000000000004,
        6.000000000000004,
        13.499999999999984,
        6.000000000000004,
        6.000000000000004,
        -1.500000000000001,
        13.499999999999984,
        6.000000000000004,
        10.09,
        13.499999999999984,
        10.09,
        13.499999999999984,
        6.000000000000004,
        6.000000000000004,
        10.09,
        6.000000000000004,
        6.000000000000004,
        -1.500000000000001,
        -1.500000000000001,
        6.000000000000004,
        6.000000000000004,
        6.000000000000004,
        13.499999999999984,
        -1.500000000000001,
        -1.500000000000001,
        -1.500000000000001,
        6.000000000000004,
        13.499999999999984,
        -1.500000000000001,
        6.000000000000004,
        -1.500000000000001,
        6.000000000000004,
        13.499999999999984,
        6.000000000000004,
        6.000000000000004,
        13.499999999999984,
        6.000000000000004,
        6.000000000000004,
        6.000000000000004,
        6.000000000000004
      ],
      "episode_lengths": [
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        1,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        1,
        150,
        1,
        150,
        150,
        150,
        1,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150
      ]
    }
  ],
  "overall_avg_reward": 3.8177,
  "overall_success_rate": 0.03,
  "best_scenario": "multi_agent_complex",
  "worst_scenario": "single_agent_complex",
  "curriculum_stats": {
    "total_tasks": 4,
    "curriculum_active": true,
    "progression_successful": true,
    "scenarios_completed": 4,
    "overall_improvement": true,
    "total_scores_recorded": 200,
    "adaptation_history_length": 4
  }
}