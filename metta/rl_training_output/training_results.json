{
  "experiment_name": "metta_rl_training_demo",
  "total_scenarios": 4,
  "episodes_per_scenario": 50,
  "scenario_results": [
    {
      "scenario_name": "single_agent_simple",
      "description": "Single agent in simple environment",
      "episodes": 50,
      "avg_reward": 2.5236,
      "std_reward": 1.8957233553448662,
      "avg_length": 48.04,
      "success_rate": 0.04,
      "best_reward": 10.09,
      "worst_reward": -0.5000000000000002,
      "reward_improvement": true,
      "training_efficiency": 0.100944,
      "episode_rewards": [
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        4.499999999999997,
        10.09,
        -0.5000000000000002,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        4.499999999999997,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        -0.5000000000000002,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        4.499999999999997,
        -0.5000000000000002,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        4.499999999999997,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        10.09,
        2.000000000000001,
        2.000000000000001,
        4.499999999999997,
        4.499999999999997,
        4.499999999999997,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001
      ],
      "episode_lengths": [
        50,
        50,
        50,
        50,
        50,
        50,
        1,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        1,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50
      ]
    },
    {
      "scenario_name": "single_agent_complex",
      "description": "Single agent in larger environment",
      "episodes": 50,
      "avg_reward": 1.5200000000000002,
      "std_reward": 2.4119701490690137,
      "avg_length": 80.0,
      "success_rate": 0.0,
      "best_reward": 7.199999999999993,
      "worst_reward": -0.8000000000000005,
      "reward_improvement": true,
      "training_efficiency": 0.0,
      "episode_rewards": [
        -0.8000000000000005,
        -0.8000000000000005,
        3.200000000000002,
        3.200000000000002,
        3.200000000000002,
        3.200000000000002,
        3.200000000000002,
        3.200000000000002,
        3.200000000000002,
        -0.8000000000000005,
        3.200000000000002,
        3.200000000000002,
        -0.8000000000000005,
        -0.8000000000000005,
        -0.8000000000000005,
        7.199999999999993,
        -0.8000000000000005,
        -0.8000000000000005,
        3.200000000000002,
        -0.8000000000000005,
        7.199999999999993,
        -0.8000000000000005,
        -0.8000000000000005,
        3.200000000000002,
        -0.8000000000000005,
        3.200000000000002,
        3.200000000000002,
        3.200000000000002,
        -0.8000000000000005,
        -0.8000000000000005,
        -0.8000000000000005,
        -0.8000000000000005,
        3.200000000000002,
        -0.8000000000000005,
        3.200000000000002,
        -0.8000000000000005,
        3.200000000000002,
        -0.8000000000000005,
        3.200000000000002,
        -0.8000000000000005,
        3.200000000000002,
        3.200000000000002,
        3.200000000000002,
        3.200000000000002,
        -0.8000000000000005,
        7.199999999999993,
        -0.8000000000000005,
        -0.8000000000000005,
        -0.8000000000000005,
        3.200000000000002
      ],
      "episode_lengths": [
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80
      ]
    },
    {
      "scenario_name": "multi_agent_simple",
      "description": "Multiple agents in simple environment",
      "episodes": 50,
      "avg_reward": 5.521799999999998,
      "std_reward": 2.5349332851181656,
      "avg_length": 98.02,
      "success_rate": 0.02,
      "best_reward": 10.09,
      "worst_reward": -1.0000000000000007,
      "reward_improvement": true,
      "training_efficiency": 0.11043599999999996,
      "episode_rewards": [
        4.000000000000003,
        4.000000000000003,
        8.999999999999991,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        8.999999999999991,
        8.999999999999991,
        8.999999999999991,
        4.000000000000003,
        -1.0000000000000007,
        8.999999999999991,
        4.000000000000003,
        4.000000000000003,
        8.999999999999991,
        8.999999999999991,
        4.000000000000003,
        8.999999999999991,
        8.999999999999991,
        10.09,
        4.000000000000003,
        8.999999999999991,
        4.000000000000003,
        8.999999999999991,
        4.000000000000003,
        8.999999999999991,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        8.999999999999991,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        8.999999999999991,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        8.999999999999991,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003
      ],
      "episode_lengths": [
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        1,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100
      ]
    },
    {
      "scenario_name": "multi_agent_complex",
      "description": "Multiple agents in complex environment",
      "episodes": 50,
      "avg_reward": 5.181800000000001,
      "std_reward": 4.462634957062921,
      "avg_length": 147.02,
      "success_rate": 0.02,
      "best_reward": 13.499999999999984,
      "worst_reward": -1.500000000000001,
      "reward_improvement": true,
      "training_efficiency": 0.10363600000000002,
      "episode_rewards": [
        -1.500000000000001,
        6.000000000000004,
        13.499999999999984,
        6.000000000000004,
        6.000000000000004,
        -1.500000000000001,
        -1.500000000000001,
        6.000000000000004,
        6.000000000000004,
        -1.500000000000001,
        6.000000000000004,
        6.000000000000004,
        -1.500000000000001,
        6.000000000000004,
        13.499999999999984,
        13.499999999999984,
        10.09,
        -1.500000000000001,
        -1.500000000000001,
        6.000000000000004,
        6.000000000000004,
        6.000000000000004,
        -1.500000000000001,
        6.000000000000004,
        6.000000000000004,
        13.499999999999984,
        6.000000000000004,
        6.000000000000004,
        6.000000000000004,
        6.000000000000004,
        6.000000000000004,
        -1.500000000000001,
        6.000000000000004,
        6.000000000000004,
        6.000000000000004,
        6.000000000000004,
        6.000000000000004,
        13.499999999999984,
        6.000000000000004,
        6.000000000000004,
        6.000000000000004,
        -1.500000000000001,
        -1.500000000000001,
        6.000000000000004,
        6.000000000000004,
        13.499999999999984,
        6.000000000000004,
        6.000000000000004,
        -1.500000000000001,
        6.000000000000004
      ],
      "episode_lengths": [
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        1,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150
      ]
    }
  ],
  "overall_avg_reward": 3.6868,
  "overall_success_rate": 0.02,
  "best_scenario": "multi_agent_simple",
  "worst_scenario": "single_agent_complex",
  "curriculum_stats": {
    "total_tasks": 4,
    "curriculum_active": true,
    "progression_successful": true,
    "scenarios_completed": 4,
    "overall_improvement": true,
    "total_scores_recorded": 200,
    "adaptation_history_length": 4
  }
}