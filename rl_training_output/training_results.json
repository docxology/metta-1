{
  "experiment_name": "metta_rl_training_demo",
  "total_scenarios": 4,
  "episodes_per_scenario": 50,
  "scenario_results": [
    {
      "scenario_name": "single_agent_simple",
      "description": "Single agent in simple environment",
      "episodes": 50,
      "avg_reward": 1.9000000000000006,
      "std_reward": 0.8602325267042625,
      "avg_length": 50.0,
      "success_rate": 0.0,
      "best_reward": 4.499999999999997,
      "worst_reward": -0.5000000000000002,
      "reward_improvement": true,
      "training_efficiency": 0.0,
      "episode_rewards": [
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        -0.5000000000000002,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        -0.5000000000000002,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        -0.5000000000000002,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        -0.5000000000000002,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        4.499999999999997,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        4.499999999999997,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001,
        2.000000000000001
      ],
      "episode_lengths": [
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50
      ]
    },
    {
      "scenario_name": "single_agent_complex",
      "description": "Single agent in larger environment",
      "episodes": 50,
      "avg_reward": 1.3378000000000005,
      "std_reward": 2.4741974779713933,
      "avg_length": 78.42,
      "success_rate": 0.02,
      "best_reward": 10.09,
      "worst_reward": -0.8000000000000005,
      "reward_improvement": true,
      "training_efficiency": 0.026756000000000012,
      "episode_rewards": [
        -0.8000000000000005,
        -0.8000000000000005,
        10.09,
        -0.8000000000000005,
        3.200000000000002,
        -0.8000000000000005,
        -0.8000000000000005,
        -0.8000000000000005,
        3.200000000000002,
        -0.8000000000000005,
        -0.8000000000000005,
        -0.8000000000000005,
        3.200000000000002,
        -0.8000000000000005,
        3.200000000000002,
        7.199999999999993,
        3.200000000000002,
        -0.8000000000000005,
        -0.8000000000000005,
        3.200000000000002,
        -0.8000000000000005,
        -0.8000000000000005,
        3.200000000000002,
        -0.8000000000000005,
        3.200000000000002,
        -0.8000000000000005,
        -0.8000000000000005,
        3.200000000000002,
        3.200000000000002,
        3.200000000000002,
        -0.8000000000000005,
        3.200000000000002,
        3.200000000000002,
        -0.8000000000000005,
        -0.8000000000000005,
        3.200000000000002,
        3.200000000000002,
        -0.8000000000000005,
        -0.8000000000000005,
        3.200000000000002,
        -0.8000000000000005,
        3.200000000000002,
        3.200000000000002,
        -0.8000000000000005,
        3.200000000000002,
        3.200000000000002,
        3.200000000000002,
        3.200000000000002,
        -0.8000000000000005,
        -0.8000000000000005
      ],
      "episode_lengths": [
        80,
        80,
        1,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80,
        80
      ]
    },
    {
      "scenario_name": "multi_agent_simple",
      "description": "Multiple agents in simple environment",
      "episodes": 50,
      "avg_reward": 5.1218,
      "std_reward": 2.3416504350564322,
      "avg_length": 98.02,
      "success_rate": 0.02,
      "best_reward": 10.09,
      "worst_reward": -1.0000000000000007,
      "reward_improvement": true,
      "training_efficiency": 0.10243600000000001,
      "episode_rewards": [
        8.999999999999991,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        8.999999999999991,
        4.000000000000003,
        8.999999999999991,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        8.999999999999991,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        10.09,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        8.999999999999991,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        -1.0000000000000007,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        8.999999999999991,
        8.999999999999991,
        8.999999999999991,
        8.999999999999991,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        4.000000000000003,
        8.999999999999991,
        4.000000000000003,
        8.999999999999991
      ],
      "episode_lengths": [
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        1,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100,
        100
      ]
    },
    {
      "scenario_name": "multi_agent_complex",
      "description": "Multiple agents in complex environment",
      "episodes": 50,
      "avg_reward": 5.700000000000001,
      "std_reward": 3.9572717874818726,
      "avg_length": 150.0,
      "success_rate": 0.0,
      "best_reward": 13.499999999999984,
      "worst_reward": -1.500000000000001,
      "reward_improvement": true,
      "training_efficiency": 0.0,
      "episode_rewards": [
        6.000000000000004,
        6.000000000000004,
        6.000000000000004,
        6.000000000000004,
        6.000000000000004,
        6.000000000000004,
        6.000000000000004,
        6.000000000000004,
        6.000000000000004,
        -1.500000000000001,
        6.000000000000004,
        -1.500000000000001,
        6.000000000000004,
        6.000000000000004,
        6.000000000000004,
        -1.500000000000001,
        13.499999999999984,
        6.000000000000004,
        6.000000000000004,
        6.000000000000004,
        6.000000000000004,
        6.000000000000004,
        6.000000000000004,
        6.000000000000004,
        6.000000000000004,
        6.000000000000004,
        -1.500000000000001,
        6.000000000000004,
        6.000000000000004,
        -1.500000000000001,
        6.000000000000004,
        13.499999999999984,
        6.000000000000004,
        6.000000000000004,
        6.000000000000004,
        6.000000000000004,
        13.499999999999984,
        6.000000000000004,
        -1.500000000000001,
        6.000000000000004,
        13.499999999999984,
        6.000000000000004,
        6.000000000000004,
        6.000000000000004,
        6.000000000000004,
        13.499999999999984,
        13.499999999999984,
        6.000000000000004,
        -1.500000000000001,
        -1.500000000000001
      ],
      "episode_lengths": [
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150,
        150
      ]
    }
  ],
  "overall_avg_reward": 3.5149000000000004,
  "overall_success_rate": 0.01,
  "best_scenario": "multi_agent_complex",
  "worst_scenario": "single_agent_complex",
  "curriculum_stats": {
    "total_tasks": 4,
    "curriculum_active": true,
    "progression_successful": true,
    "scenarios_completed": 4,
    "overall_improvement": true,
    "total_scores_recorded": 200,
    "adaptation_history_length": 4
  }
}